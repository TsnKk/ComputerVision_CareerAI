#!/usr/bin/env python3
"""
ü§ñ ai_module.py - Legacy AI Processing Engine
=============================================
‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å:
- Llama3-based AI processing (Legacy implementation)
- Thai language conversation handling
- Luna AI assistant personality implementation
- Subprocess-based model execution

‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
- AIProcessor class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Llama3 model interaction
- Thai language response generation
- English to Thai translation ‡πÅ‡∏•‡∏∞ processing
- Legacy conversation management

‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô:
- This is a LEGACY module for backward compatibility
- Current system uses Google Gemini AI instead
- Keep for migration ‡πÅ‡∏•‡∏∞ fallback purposes only
- New implementations should use core/interview_system.py

Status: DEPRECATED - Use Gemini AI in new implementations
‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: from modules.ai_module import AIProcessor (Legacy only)
=============================================
"""

import subprocess
import json

class AIProcessor:
    def __init__(self, model="llama3"):
        self.model = model

    def get_response(self, text):
        prompt = f"""
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏•‡∏π‡∏ô‡πà‡∏≤ (Luna) ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏™‡∏°‡∏≠ 
        ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≠‡∏ö
        ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∑‡∏≠: "{text}"
        """
        try:
            response = subprocess.check_output(
                ["ollama", "run", self.model],
                input=prompt.encode("utf-8"),
                text=True
            )
            return response.strip()
        except Exception as e:
            return f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}"

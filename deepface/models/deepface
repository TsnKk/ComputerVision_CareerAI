import cv2
import math
from deepface import DeepFace
import mediapipe as mp

# ---------- Config ----------
BOX_COLOR = (0, 255, 0)
TEXT_COLOR = (255, 255, 255)
TEXT_BG = (0, 0, 0)

BLINK_EAR_THRESH = 0.19
BLINK_CONSEC_FRAMES = 3

LEFT_EYE_IDX  = [33, 160, 158, 133, 153, 144]
RIGHT_EYE_IDX = [362, 385, 387, 263, 373, 380]

mp_face_mesh = mp.solutions.face_mesh

def draw_label(img, text, x, y, text_color=TEXT_COLOR, bg_color=TEXT_BG):
    font = cv2.FONT_HERSHEY_SIMPLEX
    scale, thickness = 0.7, 2
    (tw, th), baseline = cv2.getTextSize(text, font, scale, thickness)
    cv2.rectangle(img, (x, max(0, y - th - 10)), (x + tw + 10, y), bg_color, -1)
    cv2.putText(img, text, (x + 5, y - 5), font, scale, text_color, thickness, cv2.LINE_AA)

def euclid_dist(p1, p2):
    return math.hypot(p1[0] - p2[0], p1[1] - p2[1])

def eye_aspect_ratio(landmarks, idx_list, w, h):
    pts = [(landmarks[idx].x * w, landmarks[idx].y * h) for idx in idx_list]
    p1, p2, p3, p4, p5, p6 = pts
    vert = euclid_dist(p2, p6) + euclid_dist(p3, p5)
    horiz = 2.0 * euclid_dist(p1, p4)
    return vert / horiz if horiz != 0 else 0

def main():
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("❌ Cannot open camera")
        return

    blink_counter, ear_below_thresh_frames = 0, 0
    last_emotion = "unknown"
    frame_count = 0  # ใช้สำหรับ skip emotion detect

    with mp_face_mesh.FaceMesh(
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    ) as face_mesh:

        while True:
            ok, frame = cap.read()
            if not ok:
                break
            h, w = frame.shape[:2]

            # -------- Emotion detection (ทุกๆ 10 เฟรม) --------
            if frame_count % 10 == 0:
                try:
                    results = DeepFace.analyze(
                        frame,
                        actions=['emotion'],
                        detector_backend='opencv',
                        enforce_detection=False
                    )
                    if isinstance(results, list):
                        results = results[0]
                    last_emotion = results.get("dominant_emotion", "unknown")
                except Exception:
                    pass

            draw_label(frame, f"Emotion: {last_emotion}", 10, 30)

            # -------- Blink detection --------
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            res = face_mesh.process(rgb)

            if res.multi_face_landmarks:
                landmarks = res.multi_face_landmarks[0].landmark
                left_ear = eye_aspect_ratio(landmarks, LEFT_EYE_IDX, w, h)
                right_ear = eye_aspect_ratio(landmarks, RIGHT_EYE_IDX, w, h)
                ear = (left_ear + right_ear) / 2.0

                if ear < BLINK_EAR_THRESH:
                    ear_below_thresh_frames += 1
                else:
                    if ear_below_thresh_frames >= BLINK_CONSEC_FRAMES:
                        blink_counter += 1
                    ear_below_thresh_frames = 0

                draw_label(frame, f"Blinks: {blink_counter}", 10, h - 10)

            cv2.imshow("Emotion + Blink (Stable)", frame)
            frame_count += 1

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
